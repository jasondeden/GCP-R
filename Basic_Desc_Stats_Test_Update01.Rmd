---
title: "Introduction to Data Munging and Descriptive Stats starting with BigQuery Data"
author: "Jason Eden"
date: "2/26/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## You Don't Know Me. You Think You Know Me...

### Descriptive Stats on BigQuery

The purpose of this document is to go through some typical, basic data manipulation and descriptive 
statistical analysis on data in BigQuery using R and the bigrquery tool set plus basic R stats tools. 

Let's start by loading the libraries and point to our credentials. (Note: I ended up not actually doing
anything with Cloud Storage, but am leaving the reference here in case you want to expand on what I did.)

Note: I'm defining projectId as bigquery-public-data, which we'll stick with until we start creating new data for our own purposes. 

*Version 2. Credit for this and most of the rest of the improvements in this file go to Dr. Tim Wiemken.*


```{r}
library(googleAuthR)
library(googleCloudStorageR)
library(bigrquery)
library(tidyverse)
library(DT)   #Used to produce interactive tables... pretty sweet!

gar_auth_service("/Users/jeden/admkey.json", scope = "https://www.googleapis.com/auth/cloud-platform")
projectId <- "bigquery-public-data"
bq_auth(path = "/Users/jeden/admkey.json")

```

## Why Buy the Cow?

### Browse BigQuery Public Datasets. 

Let's start by collecting a list of all of the datasets available in the bigquery-public-data project. There
are a lot of them, so we'll need to choose something to focus in on. For this demo, I decided to see what I
could do with some of the public COVID-19 datasets that were out there. 

```{r}
#List datasets available in bigquery-public-data and then search that output
#for datasets with covid in the name using grep

publicdata <- bigrquery::bq_project_datasets(projectId)

covidsets <- grep("covid", publicdata, ignore.case = TRUE, value = TRUE)

#take a look at the list

covidsets

```

### Explore a Particular Dataset of Interest

OK, now we have a list of public datasets and a subset list of just datasets the are explicitly labeled covid.
Let's pick one of those and see what tables are available. 

```{r}

#get a list of the tables in one of the public covid datasets

cvdataset <- "covid19_nyt"

nyttables <- bigrquery::list_tables(projectId, cvdataset)

nyttables

```

### Explore a Particular Table in a Dataset

Let's take a look at one of those tables and see if there's anything interesting to explore.

```{r}

#create a reference to the table you want to explore

cvtable <- bigrquery::bq_table(projectId, cvdataset, "us_counties")

#download and examine the first few rows to see what's there

cvtable_head <- bigrquery::bq_table_download(cvtable, max_results = 10)

print(cvtable_head)

```

## Context is King

### Search for Data Enrichment Opportunities

It looks like we can pull some meaningful things out of this data. It contains day by day statistics on
cumulative covid cases and deaths by county. What we are missing here is scale in terms of population of each
county. I wonder if BigQuery has public data available that we could add in to give us some perspective?

```{r}

#We already have the list of public datasets, so let's look for population data. Census seems like a likely keyword. Let's try it and see what we get:

censussets <- grep("census", publicdata, ignore.case = TRUE, value = TRUE)

censussets

```

### How Long Have You Had These Droids...

Hmm. Maybe census_bureau_usa is the droid we are looking for? Let's check.

```{r}
#get a list of the tables in one of the public covid datasets

censusdataset <- "census_bureau_usa"

census_tables <- bigrquery::list_tables(projectId, censusdataset)

census_tables

```

### These Aren't the Droids You're Looking For

Hrm. Actually, not exactly what we were hoping for. I need population by county rather than by zip, and would
like to find estimates a little more recent than the 2010 data. Let's keep looking.

```{r}
#get a list of the tables in one of the public covid datasets - take 2

censusdataset <- "census_bureau_acs"

census_tables <- bigrquery::list_tables(projectId, censusdataset)

census_tables
```

### Those *ARE* the Droids!

Ah, we may have found what we're after. Let's take a look at the data in the county_2018_5yr table.

```{r}

#create a reference to the table you want to explore

census_table <- bigrquery::bq_table(projectId, censusdataset, "county_2018_5yr")

#download and examine the first few rows to see what's there

census_table_head <- bigrquery::bq_table_download(census_table, max_results = 10)

print(census_table_head)

```

## Move Along (Into Your Own Project) 

### Create a Home for Powerful Data

Aha! So we have 2018 population estimates in a field labeled total_pop, and a geo_id field which matches our
fips_code field from the covid data, which means we can join these two sets of information together.

Now, here's a cool thing - because all of this data exists in the cloud on a platform that we can manipulate,
we don't actually have to pull it down to our laptop in order to do the data munging. We can actually just tell
BigQuery to do all of the work and we won't be constrained by the limits of our local system hardware.

Let's start by creating a dataset in our own project to store the newly combined table in.

```{r}
#create a reference to your own project id

my_project_Id <- "hds5310-303717"

#create a reference to a bq_dataset -- Note, you can only create a single dataset once
#so you will need to change the dataset name for multiple runs of this code

my_covid_dataset <- bigrquery::bq_dataset(my_project_Id, "demo_covid_data_update02_update01")

bigrquery::bq_dataset_create(my_covid_dataset)

mydatasets <- bigrquery::bq_project_datasets(my_project_Id)

mydatasets

```

### Test Your Powers

Sweet. So now we have our new dataset created in our GCP project, so let's put some tables in there. I found
this easier to keep in the cloud (and not try to automatically download anything) by using SQL, but you can
also do this with dplyr tools pointing to BigQuery as a remote object as detailed here:
https://rpubs.com/shivanandiyer/BigRQuery 

The first thing I want to do is an easy table create, just to make sure I've got my credentials and everything
set up correctly. I don't plan to use this table right now.

```{r}

# Creating a simple table to test, will do the join statement once this is working correctly

SQL_Test <- "CREATE TABLE `hds5310-303717.demo_covid_data_update02.mo_beds` AS SELECT * FROM `bigquery-public-data.covid19_aha.hospital_beds` WHERE state_name = 'Missouri'"

bq_project_query(my_project_Id, SQL_Test)

#create a reference to the table you want to explore

mo_hospital_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data_update02", "mo_beds")

#download and examine the first few rows to see what's there

mo_hosp_table_head <- bigrquery::bq_table_download(mo_hospital_table, max_results = 10)

print(mo_hosp_table_head)

```

### You Have Grown Powerful. JOIN the Dark Side

Our simple test worked! Now let's get down to the real business at hand.

Note: The query below can be pseudocoded as follows - "Create a table in my dataset that select all of the
columns from the nyt public data and adds another column for the county population based on the 2018 census
estimate. You'll know what goes with what because the fips_code in the public nyt data is the same thing as the
geo_id field in the census data." 

This isn't designed to teach you how to do SQL, but if you're new to it, hopefully that helps a little.

```{r}

#This is so powerful to just run in BigQuery - if you're not a SQL coder, it's worth learning how to do just to take advantage of this remote powerhouse available to you.

SQL_nyt_join_pop <- "CREATE TABLE `hds5310-303717.demo_covid_data_update02.nyt_pl_pop` AS SELECT nyt.date, nyt.county_fips_code, nyt.county, nyt.state_name, nyt.confirmed_cases, nyt.deaths, census.total_pop FROM `bigquery-public-data.covid19_nyt.us_counties` AS nyt JOIN `bigquery-public-data.census_bureau_acs.county_2018_5yr` AS census ON nyt.county_fips_code = census.geo_id"

#Let's run this query and see if it works!

bq_project_query(my_project_Id, SQL_nyt_join_pop)

#create a reference to the table you want to explore

nyt_plus_pop_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data_update02", "nyt_pl_pop")

#download and examine the first few rows to see what's there

nyt_plus_pop_table_head <- bigrquery::bq_table_download(nyt_plus_pop_table, max_results = 10)

print(nyt_plus_pop_table_head)

```

### The Force is Strong with This One

BOOM! We've got a table that has now joined our county population data with the covid stats. Let's take a look
at some table info and see what we're working with.

```{r}

#Basic table info: size, number of rows, fields (can also do bq_meta for more explicite metadata if desired)

tsize <- bq_table_size(nyt_plus_pop_table)
trows <- bq_table_nrow(nyt_plus_pop_table)
tfields <- bq_table_fields(nyt_plus_pop_table)
tmetadata <- bq_table_meta(nyt_plus_pop_table)

print(paste0("size in MB = ", round(tsize/1000000,2)))
print(paste0("number of rows = ", trows))
print(tfields)

```

So right now we have a 61 MB table with just over 1 million rows, and now we know the data types for our fields
as well in case that's needed later (spoiler alert - it almost certainly will be...)

## Don't Fear the Factor

### Creating Factor Variables for Statistical Analysis

Let's do a little more work to make some of this data meaningful. Let's create a version of the table where 
we create a factor column based on population.
https://www.census.gov/library/stories/2017/10/big-and-small-counties.html gives us some statistics on how
"big" and "small" counties are defined: Big = 926 people / square mile, small = 48 people per square mile. It
also provides population ranges for big and small counties, and it turns out that the big counties have more
than 480,000 total population, whereas the largest "small" county is under 480,000. We can use this as an
anchor point, then, to create a factor to see if population density affects whatever it is we want to measure -
say, cases or deaths per X number of residents.

```{r}

SQL_bigsmall_eval <- "CREATE TABLE `hds5310-303717.demo_covid_data_update02.nyt_pop_den` AS SELECT *, CASE WHEN total_pop >= 480000 THEN 'dense' ELSE 'sparse' END AS pop_density FROM `hds5310-303717.demo_covid_data_update02.nyt_pl_pop`"

bq_project_query(my_project_Id, SQL_bigsmall_eval)

#create a reference to the table you want to explore

nyt_dense_eval_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data_update02", "nyt_pop_den")

#download and examine the first few rows to see what's there

nyt_pop_den_table_head <- bigrquery::bq_table_download(nyt_dense_eval_table, max_results = 10)

print(nyt_pop_den_table_head)

```

### Putting Things in Perspective

OK. Now let's add one more column that gives us a "deaths per thousand" and "cases ber thousand" fields to play
with so that we can do more like-for-like comparisons across densely and sparsely populated counties.

```{r}
SQL_dcper1k <- "CREATE TABLE `hds5310-303717.demo_covid_data_update02.nyt_dth_percap` AS SELECT *, deaths / total_pop / 1000 AS death_per_1k, confirmed_cases / total_pop / 1000 AS cases_per_1k FROM `hds5310-303717.demo_covid_data_update02.nyt_pop_den`"

bq_project_query(my_project_Id, SQL_dcper1k)

#create a reference to the table you want to explore

nyt_dcper1k_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data_update02", "nyt_dth_percap")

#download and examine the first few rows to see what's there

nyt_dcper1k_table_head <- bigrquery::bq_table_download(nyt_dcper1k_table, max_results = 10)

print(nyt_dcper1k_table_head)

```

### Univariable Analysis

So now we have some equalized values and categorical values to play with, let's take a look at some descriptive
stats that we can see before we download anything locally. Usinb BigQuery SQL, we can look at basic values like
MAX, MIN, AVG (aka "mean"), SUM, and so forth.

https://cloud.google.com/bigquery/docs/reference/standard-sql/aggregate_functions


```{r}

max_cases <- bq_project_query(my_project_Id, "SELECT MAX(confirmed_cases) FROM `hds5310-303717.demo_covid_data_update02.nyt_dth_percap`")

min_cases <- bq_project_query(my_project_Id, "SELECT MIN(confirmed_cases) FROM `hds5310-303717.demo_covid_data_update02.nyt_dth_percap`")

mean_cases <- bq_project_query(my_project_Id, "SELECT AVG(confirmed_cases) FROM `hds5310-303717.demo_covid_data_update02.nyt_dth_percap`")

bq_table_download(max_cases)
bq_table_download(min_cases)
bq_table_download(mean_cases)

```

## Take Things Day by Day

### Subset Data to a Point In Time

It occurs to me that my table - being more than a million rows - is collecting daily, cumulative data rather
than data for a given day, and that's going to throw off some of what we might want to look at, unless we want
to join this data with things like weather data by county and so forth. Therefore, for now, let's create a
table that represents the subset of our data collected as of February 15, 2021. This will make future queries
where we are interested in point in time rather than time series cheaper as well.

```{r}
SQL_Feb152021 <- "CREATE TABLE `hds5310-303717.demo_covid_data_update02.nyt_Feb152021` AS SELECT * FROM `hds5310-303717.demo_covid_data_update02.nyt_dth_percap` WHERE date = '2021-02-15'"

bq_project_query(my_project_Id, SQL_Feb152021)

#create a reference to the table you want to explore

nyt_Feb152021_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data_update02", "nyt_Feb152021")

#download and examine the first few rows to see what's there

nyt_Feb152021_table_head <- bigrquery::bq_table_download(nyt_Feb152021_table, max_results = 10)

print(nyt_Feb152021_table_head)

```

Check our table info again:

```{r}
#Basic table info: size, number of rows, fields (can also do bq_meta for more explicite metadata if desired)

tsize2 <- bq_table_size(nyt_Feb152021_table)
trows2 <- bq_table_nrow(nyt_Feb152021_table)
tfields2 <- bq_table_fields(nyt_Feb152021_table)
tmetadata2 <- bq_table_meta(nyt_Feb152021_table)

print(paste0("size in MB = ", round(tsize2/1000000,2)))
print(paste0("number of rows = ", trows2))
print(tfields2)

```


Muuuuuch smaller data now, with 3,211 total counties. Let's see how this point in time view will affect our
aggregate stats:

```{r}
max_cases2 <- bq_project_query(my_project_Id, "SELECT MAX(confirmed_cases) FROM `hds5310-303717.demo_covid_data_update02.nyt_Feb152021`")

min_cases2 <- bq_project_query(my_project_Id, "SELECT MIN(confirmed_cases) FROM `hds5310-303717.demo_covid_data_update02.nyt_Feb152021`")

mean_cases2 <- bq_project_query(my_project_Id, "SELECT AVG(confirmed_cases) FROM `hds5310-303717.demo_covid_data_update02.nyt_Feb152021`")

bq_table_download(max_cases2)
bq_table_download(min_cases2)
bq_table_download(mean_cases2)

```

### Get It Where It COUNTs

So the MAX was a little lower, but the MIN did not go up. (I wonder what the population of the county with only
one confirmed case as of Feb 15, 2021 is?) But look at the mean - significantly higher than before, which you
would expect since we took out smaller numbers. 

Now we're able to do some additional analysis using SQL. How many counties are dense vs. sparsely populated? 

```{r}
dense_count <- bq_project_query(my_project_Id, "SELECT COUNT(pop_density) FROM `hds5310-303717.demo_covid_data_update02.nyt_Feb152021` WHERE pop_density = 'dense'")

total_count <- bq_project_query(my_project_Id, "SELECT COUNT(pop_density) FROM `hds5310-303717.demo_covid_data_update02.nyt_Feb152021`")

bq_table_download(dense_count)
bq_table_download(total_count)

```

So we can see that of the 3,211 total counties, 139 of them are densely populated (by our definition). 

## Bring It Home

### Shop Local

Up till now, we've been working with the data in BigQuery to avoid using local system compute. However, I think
the data are small enough now that we can download it and start doing things locally without putting too much
stress on our system.

(Added datatable)

```{r}

bqlocal <- bq_table_download(nyt_Feb152021_table)

localdata <- data.table::as.data.table(bqlocal)

#print(localdata)

data.table(head(localdata))  #This is from the DT package added in the first code chunk

```

### Table for Two (County Types)

Now let's do some descriptive statistical analysis. 

```{r}
table(localdata$pop_density)

```


```{r}
prop.table(table(localdata$pop_density))

```

### To SUM It All Up...

OK, so shocker, there counties are heavily weighted towards sparse populations vs. densely
populated areas. I'm curious though - how does the overall population break out then?

(My old code used a loop to accomplish this. Has been updated to use a much better subsetting approach. Not 
only does it code cleaner / easier, it would run significantly faster, especially if we were using a really 
large data set. 

I also suspect Dr. Wiemken was showing off, just a touch. :) 


```{r}
# THIS CODE HAS BEEN REPLACED
#  dense_total_pop <- 0
#  sparse_total_pop <- 0
# 
#  for (x in c(1:nrow(localdata))){
#    if (localdata$pop_density[x] == 'sparse'){
#      sparse_total_pop <- sparse_total_pop + localdata$total_pop[x]}
#    else{dense_total_pop <- dense_total_pop + localdata$total_pop[x]}
#  }
# 
#  print(paste0("dense county total population = ", dense_total_pop))
#  print(paste0("sparse county total population = ", sparse_total_pop))
# 
```

```{r}

#Do it in an easier / better / more efficient / faster way
#Subsetting / data slicing for the win!

sparse_total_pop <- sum(localdata$total_pop[localdata$pop_density=='sparse'], na.rm=T)

dense_total_pop <- sum(localdata$total_pop[localdata$pop_density=='dense'], na.rm=T)

print(paste0("dense county total population = ", dense_total_pop))
print(paste0("sparse county total population = ", sparse_total_pop))

```

Interesting. So the population in sparse vs. densely populated counties is pretty close to the same. This may
come in handy later. Let's do a couple more like this and check the difference between cases and deaths in the
dense vs. sparse populations.

(Again, vastly improved method below - leaving my previous code in for comparison / posterity.)

```{r}
#THIS CODE HAS BEEN REPLACED  
# dense_total_deaths <- 0
# dense_total_cases <- 0
# sparse_total_deaths <- 0
# sparse_total_cases <- 0 
# 
# for (x in c(1:nrow(localdata))){
#   if (localdata$pop_density[x] == 'sparse'){
#     if (is.na(localdata$deaths[x])){  #had to add this in because there are apparently missing values
#       localdata$deaths[x] <- 0}
#     sparse_total_deaths <- sparse_total_deaths + localdata$deaths[x]
#     sparse_total_cases <- sparse_total_cases + localdata$confirmed_cases[x]}
#   else{
#     dense_total_deaths <- dense_total_deaths + localdata$deaths[x]
#     dense_total_cases <- dense_total_cases + localdata$confirmed_cases[x]}
# }
# 
# print(paste0("dense county total population = ", dense_total_pop))
# print(paste0("dense county total cases = ", dense_total_cases))
# print(paste0("dense county total deaths = ", dense_total_deaths))
# print(paste0("sparse county total population = ", sparse_total_pop))
# print(paste0("sparse county total cases = ", sparse_total_cases))
# print(paste0("sparse county total deaths = ", sparse_total_deaths))
# 
```


```{r}
# So pretty... Added the format function to add the commas to make numbers easier to read to boot.

dense_total_deaths <- sum(localdata$deaths[localdata$pop_density=='dense'], na.rm = T)
sparse_total_deaths <- sum(localdata$deaths[localdata$pop_density=='sparse'], na.rm = T)
dense_total_cases <- sum(localdata$confirmed_cases[localdata$pop_density=='dense'], na.rm = T)
sparse_total_cases <- sum(localdata$confirmed_cases[localdata$pop_density=='sparse'], na.rm = T)

print(paste0("dense county total population = ", format(dense_total_pop,big.mark = ",")))
print(paste0("dense county total cases = ", format(dense_total_cases,big.mark = ",")))
print(paste0("dense county total deaths = ", format(dense_total_deaths,big.mark = ",")))
print(paste0("sparse county total population = ", format(sparse_total_pop,big.mark = ",")))
print(paste0("sparse county total cases = ", format(sparse_total_cases,big.mark = ",")))
print(paste0("sparse county total deaths = ", format(sparse_total_deaths,big.mark = ",")))

```

## Test Your Assumptions

OK, so at a quick glance, the numbers of the total population between dense and sparse populated counties don't
appear to be that different. But in my experience, humans - including, and sometemes especially me - are
inherently bad at statistical thinking. I'm not sure whether I can trust my initial judgement here or not.
Let's run some analysis and see if our eyes deceive us or not.

To determine if we can use a parametric test for analysis, we need to see if our data are normally distributed.
The value I'm interested in will be either death rate or case rate per total population. 

### Normality Test: Histogram

Let's look at a quick histogram of the deaths and cases per 1k columns and see if we are dealing with a normal
distribution or not.

```{r}
hist(localdata$death_per_1k)

```

```{r}
hist(localdata$cases_per_1k)

```

In both cases, our data skew pretty heavily to the left, so tests like t-test and anova are **not** going to be
appropriate measures for this data. 

### Normality Test: Shapiro-Wilk

We can further validate our visual assessment with the Shapiro-Wilk normality test. This test produces a
p-value, and if that p-value is less than 0.05, we conclude that the data is **not** normally distributed. 

```{r}

shapiro.test(localdata$death_per_1k)

```

Our p-value is a lot lower than 0.05, so we can conclude with a significant degree of certainty that our data
is not normally distributed.

### Normality Test: Kurtosis

Another test we can run is to check for kurtosis - a measure of whether data are heavily tailed or light tailed relative to normal distribution. If our data tails as we would expect in a normal distribution, this will produce a value that is close to 0, and for evaluation purposes, should fit within a value of -2 to 2. Let's look at our cases per 1k and see what we get:

```{r}
library(e1071)

e1071::kurtosis(localdata$cases_per_1k)

```

A value of 3 is outside the boundaries, so this further confirms for us our data are not normally distributed.

### Normality Test: Skewness

Another test is to look at skewness. Again, we are looking for a value of -2 to 2, which would indicate that
our data is relatively symmetrical.

```{r}

e1071::skewness(localdata$cases_per_1k)

```

I ran this one to show the value of evaluating the data from multiple angles to determine normal distribution. According to the skewness test, our data does pass for this test of normal distribution, which should not be a surprise given our histogram (other than the shift to the left and the really long tail to the right, it did have symmetry to it). In other words, if we look at our mean and median values, they should be fairly similar. Let's check:

```{r}

a <- mean(localdata$cases_per_1k)
b <- median(localdata$cases_per_1k)

print(a)
print(b)

```

So as you can see, our data for cases per 1k does actually a mean and median value that are pretty similar.
Still, failing kurtosis and our basic visual check is enough here to rule out normal distribution of the data.
Therefore, we cannot use parametric statistical tests to evaluate them.

This got me interested in looking at what other types of normality tests existed, and I'm included a few more
that I found interesting.

### Normality Test: Density Plot (Histogram alternative)

Density plot - does your data have a bell-shaped curve?

```{r}
library(ggpubr)

ggpubr::ggdensity(localdata$cases_per_1k)

```

### Normality Test: Q-Q Plot

Q-Q plot (quantile-quantile plot) - do my data fit a normal distribution? (Note the outliers off the line 
at the end.)

```{r}

ggpubr::ggqqplot(localdata$cases_per_1k)


```


## Pick the Right Kind of Test

In my original effort, I used t-test and anova to try to do a bivariable analysis. Given the lack of normal
distribution in our data, this was not the optimal approach. So I went to my professor for guidance on the
next step.

### Poisson Regression Model

From Dr. Wiemken:

"The ‘most legit’ approach would be to use a univariable poisson regression model with an offset for the denominator – the outcome is just case counts. So instead of calculating the rate and then testing differences in the standardized rate between density groups, you keep it separate."

It executes as follows:

```{r}

deathresults <- glm(localdata$deaths ~ localdata$pop_density + offset(log(localdata$total_pop)), family='poisson')
summary(deathresults)

caseresults <- glm(localdata$confirmed_cases ~ localdata$pop_density + offset(log(localdata$total_pop)), family='poisson')
summary(caseresults)

```

### Interpreting the Results

There are two points of interest in these results for each test - the p-variable and the estimate.

A p-value of less than 0.05 indicates that our results are statistically significant. For both tests, our
p-value is waaaay below 0.05, so we **are** seeing a difference between sparse and densely populated counties.

The Estimate value is what tells us what the expected difference between our variables (sparse vs. dense) would
be. Since the value returned is actually a log value, we need to calculate the exponential to determine what
the effect is.

```{r}
exp(-0.009396)
exp(-0.0334286)

```

The way we interpret that is as follows: For individuals living in sparsely populated counties, you have a 99%
chance of dying of COVID-19 vs. individuals living in a densely populated county. This would align with our
base understanding of how diseases spread, perhaps - the closer together people are, the easier it is for a
virus to transmit from one person to another. So a drop in the rates of transmission and death are to be
expected.

What is interesting, and bears further analysis, is that while your chances of dying of COVID-19 are 1% lower
in sparsely populated counties, your chances of testing positive for COVID-19 in the first place are actually
about 3.5% lower. (96.7% vs. densely populated counties.) So if you test positive for COVID-19 in a sparsely
populated county, you're actually slightly more likely to die from it. Again, fascinating.

### Confidence Interval

The confidence interval....

```{r}

summary(summary(deathresults))

```


```{r}

exp(confint(deathresults))
exp(confint(caseresults))

```



## OLD - DO NOT USE

Again, leaving here for reference and posterity.

### The Null Hypothesis: Population Density Doesn't Matter

We start with the null hypothesis, which in this case would state "Population density does **not** affect the
number of cases (or deaths - we'll try to test both) per 1,000 people in a county." We'll start by running a
Welch Two Sample t-test and look at number of deaths per 1,000. 

# ```{r}
# #Does population density affect deaths per 1000?
# 
# t.test(localdata$death_per_1k~localdata$pop_density)
# 
# welchres <- t.test(localdata$death_per_1k~localdata$pop_density)
# str(welchres)
# 
# if(welchres$p.value < .05){
#   print(paste0("Null Hypothesis is Rejected by p-value of ", welchres$p.value))
# } else {print("Null Hypothesis is Accepted")}
# 
# ```

### Rejecting the Null: Population Density DOES Matter!??!

(Or, it correlates anyway, and maybe not the way you might have assumed!)

Interesting. So the p-value is less than .05, meaning we should reject our null hypothesis / initial
assumption, assuming I have set this up correctly (which is by far not guaranteed... I'm still learning here,
so probably missed something.) There does appear - based on this - to be a significant difference in the rate
of deaths per 1k population that associates with density of the population in a county. The difference between
the mean deaths per 1k between the two groups turns out to be around 13%. Assuming I haven't missed something
based on a lack of experience (which again, I probably have...) this **does** turn out to have a significant
association to overall death rate, with sparsely populated counties having ***higher*** death rates.

Did this surprise you? It might be reasonable to assume that viruses would do better with more densely packed
populations. Therefore, I would think there has to be something at work here - perhaps things like attitudes
towards masks, weather patterns, or maybe lack of healthcare resources readily available - that makes the death
rate skew higher for those counties. Association is not causation, but it does potentially indicate something
worth exploring further, especially if it's counter-intuitive. Fascinating stuff!

Let's try an anova test (analysis of variance) and see if we get similar results:

# ```{r}
# aov(localdata$death_per_1k~localdata$pop_density)
# 
# aovres <- aov(localdata$death_per_1k~localdata$pop_density)
# summary(aovres)
# 
# ```


The anova test provided a p-value of 0.0153, which is still well below our 0.05 threshold. Once again, we reject the null hypothesis. 

## Disclaimer

### This is a POC, Not Quite Real Science, Yet...

I haven't done some basic things here like checking to see if my data are normally distributed and probably
other things. The point of this exercise was to do something that resembles what a person might do in the real
world starting with bigger data in the cloud, then condensing it and bringing it to the local system for more
in-depth analysis using R tools. Think of this as a coding practice more than actually doing descriptive
statistics right. I'm sure I'll get better at that as I progress.

