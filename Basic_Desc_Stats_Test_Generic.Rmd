---
title: "Introduction to Data Munging and Descriptive Stats starting with BigQuery Data"
author: "Jason Eden"
date: "2/26/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## You Don't Know Me. You Think You Know Me...

### Descriptive Stats on BigQuery

The purpose of this document is to go through some typical, basic data manipulation and descriptive 
statistical analysis on data in BigQuery using R and the bigrquery tool set plus basic R stats tools. 

Let's start by loading the libraries and point to our credentials. (Note: I ended up not actually doing
anything with Cloud Storage, but am leaving the reference here in case you want to expand on what I did.)

Note: I'm defining projectId as bigquery-public-data, which we'll stick with until we start creating new data for our own purposes. 

```{r}
library(googleAuthR)
library(googleCloudStorageR)
library(bigrquery)
library(tidyverse)

gar_auth_service("/<path>/<key>.json", scope = "https://www.googleapis.com/auth/cloud-platform")
projectId <- "bigquery-public-data"
bq_auth(path = "/<path>/<key>.json")

```

## Why Buy the Cow?

### Browse BigQuery Public Datasets. 

Let's start by collecting a list of all of the datasets available in the bigquery-public-data project. There
are a lot of them, so we'll need to choose something to focus in on. For this demo, I decided to see what I
could do with some of the public COVID-19 datasets that were out there. 

```{r}
#List datasets available in bigquery-public-data and then search that output
#for datasets with covid in the name using grep

publicdata <- bigrquery::bq_project_datasets(projectId)

covidsets <- grep("covid", publicdata, ignore.case = TRUE, value = TRUE)

#take a look at the list

covidsets

```

### Explore a Particular Dataset of Interest

OK, now we have a list of public datasets and a subset list of just datasets the are explicitly labeled covid.
Let's pick one of those and see what tables are available. 

```{r}

#get a list of the tables in one of the public covid datasets

cvdataset <- "covid19_nyt"

nyttables <- bigrquery::list_tables(projectId, cvdataset)

nyttables

```

### Explore a Particular Table in a Dataset

Let's take a look at one of those tables and see if there's anything interesting to explore.

```{r}

#create a reference to the table you want to explore

cvtable <- bigrquery::bq_table(projectId, cvdataset, "us_counties")

#download and examine the first few rows to see what's there

cvtable_head <- bigrquery::bq_table_download(cvtable, max_results = 10)

print(cvtable_head)

```

## Context is King

### Search for Data Enrichment Opportunities

It looks like we can pull some meaningful things out of this data. It contains day by day statistics on
cumulative covid cases and deaths by county. What we are missing here is scale in terms of population of each
county. I wonder if BigQuery has public data available that we could add in to give us some perspective?

```{r}

#We already have the list of public datasets, so let's look for population data. Census seems like a likely keyword. Let's try it and see what we get:

censussets <- grep("census", publicdata, ignore.case = TRUE, value = TRUE)

censussets

```

### How Long Have You Had These Droids...

Hmm. Maybe census_bureau_usa is the droid we are looking for? Let's check.

```{r}
#get a list of the tables in one of the public covid datasets

censusdataset <- "census_bureau_usa"

census_tables <- bigrquery::list_tables(projectId, censusdataset)

census_tables

```

### These Aren't the Droids You're Looking For

Hrm. Actually, not exactly what we were hoping for. I need population by county rather than by zip, and would
like to find estimates a little more recent than the 2010 data. Let's keep looking.

```{r}
#get a list of the tables in one of the public covid datasets - take 2

censusdataset <- "census_bureau_acs"

census_tables <- bigrquery::list_tables(projectId, censusdataset)

census_tables
```

### Those *ARE* the Droids!

Ah, we may have found what we're after. Let's take a look at the data in the county_2018_5yr table.

```{r}

#create a reference to the table you want to explore

census_table <- bigrquery::bq_table(projectId, censusdataset, "county_2018_5yr")

#download and examine the first few rows to see what's there

census_table_head <- bigrquery::bq_table_download(census_table, max_results = 10)

print(census_table_head)


```

## Move Along (Into Your Own Project) 

### Create a Home for Powerful Data

Aha! So we have 2018 population estimates in a field labeled total_pop, and a geo_id field which matches our
fips_code field from the covid data, which means we can join these two sets of information together.

Now, here's a cool thing - because all of this data exists in the cloud on a platform that we can manipulate,
we don't actually have to pull it down to our laptop in order to do the data munging. We can actually just tell
BigQuery to do all of the work and we won't be constrained by the limits of our local system hardware.

Let's start by creating a dataset in our own project to store the newly combined table in.

```{r}
#create a reference to your own project id

my_project_Id <- "<your-project-id>"

#create a reference to a bq_dataset -- Note, you can only create a single dataset once
#so you will need to change the dataset name for multiple runs of this code

my_covid_dataset <- bigrquery::bq_dataset(my_project_Id, "demo_covid_data")

bigrquery::bq_dataset_create(my_covid_dataset)

mydatasets <- bigrquery::bq_project_datasets(my_project_Id)

mydatasets

```

### Test Your Powers

Sweet. So now we have our new dataset created in our GCP project, so let's put some tables in there. I found
this easier to keep in the cloud (and not try to automatically download anything) by using SQL, but you can
also do this with dplyr tools pointing to BigQuery as a remote object as detailed here:
https://rpubs.com/shivanandiyer/BigRQuery 

The first thing I want to do is an easy table create, just to make sure I've got my credentials and everything
set up correctly. I don't plan to use this table right now.

```{r}

# Creating a simple table to test, will do the join statement once this is working correctly

SQL_Test <- "CREATE TABLE `<your-project-id>.demo_covid_data.mo_beds` AS SELECT * FROM `bigquery-public-data.covid19_aha.hospital_beds` WHERE state_name = 'Missouri'"

bq_project_query(my_project_Id, SQL_Test)

#create a reference to the table you want to explore

mo_hospital_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data", "mo_beds")

#download and examine the first few rows to see what's there

mo_hosp_table_head <- bigrquery::bq_table_download(mo_hospital_table, max_results = 10)

print(mo_hosp_table_head)


```

### You Have Grown Powerful. JOIN the Dark Side

Our simple test worked! Now let's get down to the real business at hand.

Note: The query below can be pseudocoded as follows - "Create a table in my dataset that select all of the columns from the nyt public data and adds another column for the county population based on the 2018 census estimate. You'll know what goes with what because the fips_code in the public nyt data is the same thing as the geo_id field in the census data." 

This isn't designed to teach you how to do SQL, but if you're new to it, hopefully that helps a little.

```{r}

#This is so powerful to just run in BigQuery - if you're not a SQL coder, it's worth learning how to do just to take advantage of this remote powerhouse available to you.

SQL_nyt_join_pop <- "CREATE TABLE `<your-project-id>.demo_covid_data.nyt_pl_pop` AS SELECT nyt.date, nyt.county_fips_code, nyt.county, nyt.state_name, nyt.confirmed_cases, nyt.deaths, census.total_pop FROM `bigquery-public-data.covid19_nyt.us_counties` AS nyt JOIN `bigquery-public-data.census_bureau_acs.county_2018_5yr` AS census ON nyt.county_fips_code = census.geo_id"

#Let's run this query and see if it works!

bq_project_query(my_project_Id, SQL_nyt_join_pop)

#create a reference to the table you want to explore

nyt_plus_pop_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data", "nyt_pl_pop")

#download and examine the first few rows to see what's there

nyt_plus_pop_table_head <- bigrquery::bq_table_download(nyt_plus_pop_table, max_results = 10)

print(nyt_plus_pop_table_head)


```

### The Force is Strong with This One

BOOM! We've got a table that has now joined our county population data with the covid stats. Let's take a look at some table info and see what we're working with.

```{r}

#Basic table info: size, number of rows, fields (can also do bq_meta for more explicite metadata if desired)

tsize <- bq_table_size(nyt_plus_pop_table)
trows <- bq_table_nrow(nyt_plus_pop_table)
tfields <- bq_table_fields(nyt_plus_pop_table)
tmetadata <- bq_table_meta(nyt_plus_pop_table)

print(paste0("size in MB = ", round(tsize/1000000,2)))
print(paste0("number of rows = ", trows))
print(tfields)

```

So right now we have a 61 MB table with just over 1 million rows, and now we know the data types for our fields as well in case that's needed later (spoiler alert - it almost certainly will be...)

## Don't Fear the Factor

### Creating Factor Variables for Statistical Analysis

Let's do a little more work to make some of this data meaningful. Let's create a version of the table where 
we create a factor column based on population.
https://www.census.gov/library/stories/2017/10/big-and-small-counties.html gives us some statistics on how
"big" and "small" counties are defined: Big = 926 people / square mile, small = 48 people per square mile. It
also provides population ranges for big and small counties, and it turns out that the big counties have more
than 480,000 total population, whereas the largest "small" county is under 480,000. We can use this as an
anchor point, then, to create a factor to see if population density affects whatever it is we want to measure -
say, cases or deaths per X number of residents.

```{r}

SQL_bigsmall_eval <- "CREATE TABLE `<your-project-id>.demo_covid_data.nyt_pop_den` AS SELECT *, CASE WHEN total_pop >= 480000 THEN 'dense' ELSE 'sparse' END AS pop_density FROM `<your-project-id>.demo_covid_data.nyt_pl_pop`"

bq_project_query(my_project_Id, SQL_bigsmall_eval)

#create a reference to the table you want to explore

nyt_dense_eval_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data", "nyt_pop_den")

#download and examine the first few rows to see what's there

nyt_pop_den_table_head <- bigrquery::bq_table_download(nyt_dense_eval_table, max_results = 10)

print(nyt_pop_den_table_head)


```

### Putting Things in Perspective

OK. Now let's add one more column that gives us a "deaths per thousand" and "cases ber thousand" fields to play
with so that we can do more like-for-like comparisons across densely and sparsely populated counties.

```{r}
SQL_dcper1k <- "CREATE TABLE `<your-project-id>.demo_covid_data.nyt_dth_percap` AS SELECT *, deaths / total_pop / 1000 AS death_per_1k, confirmed_cases / total_pop / 1000 AS cases_per_1k FROM `<your-project-id>.demo_covid_data.nyt_pop_den`"

bq_project_query(my_project_Id, SQL_dcper1k)

#create a reference to the table you want to explore

nyt_dcper1k_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data", "nyt_dth_percap")

#download and examine the first few rows to see what's there

nyt_dcper1k_table_head <- bigrquery::bq_table_download(nyt_dcper1k_table, max_results = 10)

print(nyt_dcper1k_table_head)

```

### Univariable Analysis

So now we have some equalized values and categorical values to play with, let's take a look at some descriptive
stats that we can see before we download anything locally. Usinb BigQuery SQL, we can look at basic values like
MAX, MIN, AVG (aka "mean"), SUM, and so forth.

https://cloud.google.com/bigquery/docs/reference/standard-sql/aggregate_functions


```{r}

max_cases <- bq_project_query(my_project_Id, "SELECT MAX(confirmed_cases) FROM `<your-project-id>.demo_covid_data.nyt_dth_percap`")

min_cases <- bq_project_query(my_project_Id, "SELECT MIN(confirmed_cases) FROM `<your-project-id>.demo_covid_data.nyt_dth_percap`")

mean_cases <- bq_project_query(my_project_Id, "SELECT AVG(confirmed_cases) FROM `<your-project-id>.demo_covid_data.nyt_dth_percap`")

bq_table_download(max_cases)
bq_table_download(min_cases)
bq_table_download(mean_cases)

```

## Take Things Day by Day

### Subset Data to a Point In Time

It occurs to me that my table - being more than a million rows - is collecting daily, cumulative data rather
than data for a given day, and that's going to throw off some of what we might want to look at, unless we want
to join this data with things like weather data by county and so forth. Therefore, for now, let's create a
table that represents the subset of our data collected as of February 15, 2021. This will make future queries
where we are interested in point in time rather than time series cheaper as well.

```{r}
SQL_Feb152021 <- "CREATE TABLE `<your-project-id>.demo_covid_data.nyt_Feb152021` AS SELECT * FROM `<your-project-id>.demo_covid_data.nyt_dth_percap` WHERE date = '2021-02-15'"

bq_project_query(my_project_Id, SQL_Feb152021)

#create a reference to the table you want to explore

nyt_Feb152021_table <- bigrquery::bq_table(my_project_Id, "demo_covid_data", "nyt_Feb152021")

#download and examine the first few rows to see what's there

nyt_Feb152021_table_head <- bigrquery::bq_table_download(nyt_Feb152021_table, max_results = 10)

print(nyt_Feb152021_table_head)


```

Check our table info again:

```{r}
#Basic table info: size, number of rows, fields (can also do bq_meta for more explicite metadata if desired)

tsize2 <- bq_table_size(nyt_Feb152021_table)
trows2 <- bq_table_nrow(nyt_Feb152021_table)
tfields2 <- bq_table_fields(nyt_Feb152021_table)
tmetadata2 <- bq_table_meta(nyt_Feb152021_table)

print(paste0("size in MB = ", round(tsize2/1000000,2)))
print(paste0("number of rows = ", trows2))
print(tfields2)

```


Muuuuuch smaller data now, with 3,211 total counties. Let's see how this point in time view will affect our
aggregate stats:

```{r}
max_cases2 <- bq_project_query(my_project_Id, "SELECT MAX(confirmed_cases) FROM `<your-project-id>.demo_covid_data.nyt_Feb152021`")

min_cases2 <- bq_project_query(my_project_Id, "SELECT MIN(confirmed_cases) FROM `<your-project-id>.demo_covid_data.nyt_Feb152021`")

mean_cases2 <- bq_project_query(my_project_Id, "SELECT AVG(confirmed_cases) FROM `<your-project-id>.demo_covid_data.nyt_Feb152021`")

bq_table_download(max_cases2)
bq_table_download(min_cases2)
bq_table_download(mean_cases2)


```

### Get It Where It COUNTs

So the MAX was a little lower, but the MIN did not go up. (I wonder what the population of the county with only
one confirmed case as of Feb 15, 2021 is?) But look at the mean - significantly higher than before, which you
would expect since we took out smaller numbers. 

Now we're able to do some additional analysis using SQL. How many counties are dense vs. sparsely populated? 

```{r}
dense_count <- bq_project_query(my_project_Id, "SELECT COUNT(pop_density) FROM `<your-project-id>.demo_covid_data.nyt_Feb152021` WHERE pop_density = 'dense'")

total_count <- bq_project_query(my_project_Id, "SELECT COUNT(pop_density) FROM `<your-project-id>.demo_covid_data.nyt_Feb152021`")

bq_table_download(dense_count)
bq_table_download(total_count)

```

So we can see that of the 3,211 total counties, 139 of them are densely populated (by our definition). 

## Bring It Home

### Shop Local

Up till now, we've been working with the data in BigQuery to avoid using local system compute. However, I think
the data are small enough now that we can download it and start doing things locally without putting too much
stress on our system.

```{r}

bqlocal <- bq_table_download(nyt_Feb152021_table)

localdata <- data.table::as.data.table(bqlocal)

print(localdata)

```

### Table for Two (County Types)

Now let's do some descriptive statistical analysis. 

```{r}
table(localdata$pop_density)
```


```{r}
prop.table(table(localdata$pop_density))
```

### To SUM It All Up...

OK, so shocker, there counties are heavily weighted towards sparse populations vs. densely populated areas. I'm
curious though - how does the overall population break out then?

```{r}

dense_total_pop <- 0
sparse_total_pop <- 0

for (x in c(1:nrow(localdata))){
  if (localdata$pop_density[x] == 'sparse'){
    sparse_total_pop <- sparse_total_pop + localdata$total_pop[x]}
  else{dense_total_pop <- dense_total_pop + localdata$total_pop[x]}
}

print(paste0("dense county total population = ", dense_total_pop))
print(paste0("sparse county total population = ", sparse_total_pop))

```

Interesting. So the population in sparse vs. densely populated counties is pretty close to the same. This may
come in handy later. Let's do a couple more like this and check the difference between cases and deaths in the
dense vs. sparse populations.

```{r}

dense_total_deaths <- 0
dense_total_cases <- 0
sparse_total_deaths <- 0
sparse_total_cases <- 0 

for (x in c(1:nrow(localdata))){
  if (localdata$pop_density[x] == 'sparse'){
    if (is.na(localdata$deaths[x])){  #had to add this in because there are apparently missing values
      localdata$deaths[x] <- 0}
    sparse_total_deaths <- sparse_total_deaths + localdata$deaths[x]
    sparse_total_cases <- sparse_total_cases + localdata$confirmed_cases[x]}
  else{
    dense_total_deaths <- dense_total_deaths + localdata$deaths[x]
    dense_total_cases <- dense_total_cases + localdata$confirmed_cases[x]}
}

print(paste0("dense county total population = ", dense_total_pop))
print(paste0("dense county total cases = ", dense_total_cases))
print(paste0("dense county total deaths = ", dense_total_deaths))
print(paste0("sparse county total population = ", sparse_total_pop))
print(paste0("sparse county total cases = ", sparse_total_cases))
print(paste0("sparse county total deaths = ", sparse_total_deaths))

```

## Test Your Assumptions

### The Null Hypothesis: Population Density Doesn't Matter

OK, so at a quick glance, the numbers of the total population between dense and sparse populated counties don't
appear to be that different. But in my experience, humans - including, and sometemes especially me - are
inherently bad at statistical thinking. I'm not sure whether I can trust my initial judgement here or not.
Let's run some analysis and see if our eyes deceive us or not.

We start with the null hypothesis, which in this case would state "Population density does **not** affect the
number of cases (or deaths - we'll try to test both) per 1,000 people in a county." We'll start by running a
Welch Two Sample t-test and look at number of deaths per 1,000. 

```{r}
#Does population density affect deaths per 1000?

t.test(localdata$death_per_1k~localdata$pop_density)

welchres <- t.test(localdata$death_per_1k~localdata$pop_density)
str(welchres)

if(welchres$p.value < .05){
  print(paste0("Null Hypothesis is Rejected by p-value of ", welchres$p.value))
} else {print("Null Hypothesis is Accepted")}

```

### Rejecting the Null: Population Density DOES Matter!??!

(Or, it correlates anyway, and maybe not the way you might have assumed!)

Interesting. So the p-value is greater than 95%, meaning we should reject our null hypothesis / initial
assumption, assuming I have set this up correctly (which is by far not guaranteed... I'm still learning here,
so probably missed something.) There does appear - based on this - to be a significant difference in the rate
of deaths per 1k population that correlates with density of the population in a county. The difference between
the mean deaths per 1k between the two groups turns out to be around 13%. Assuming I haven't missed something
based on a lack of experience (which again, I probably have...) this **does** turn out to have a significant
correlation to overall death rate, with sparsely populated counties having ***higher*** death rates.

Did this surprise you? It might be reasonable to assume that viruses would do better with more densely packed
populations. Therefore, I would think there has to be something at work here - perhaps things like attitudes
towards masks, weather patterns, or maybe lack of healthcare resources readily available - that makes the death
rate skew higher for those counties. Correlation is not causation, but it does potentially indicate something
worth exploring further, especially if it's counter-intuitive. Fascinating stuff!

Let's try an anova test (analysis of variance) and see if we get similar results:

```{r}
aov(localdata$death_per_1k~localdata$pop_density)

aovres <- aov(localdata$death_per_1k~localdata$pop_density)
summary(aovres)

```


The anova test provided a p-value of 0.0153, which is still well below our 0.05 threshold. Once again, we reject the null hypothesis. 

## Disclaimer

### This is a POC, Not Quite Real Science, Yet...

I haven't done some basic things here like checking to see if my data are normally distributed and probably
other things. The point of this exercise was to do something that resembles what a person might do in the real
world starting with bigger data in the cloud, then condensing it and bringing it to the local system for more
in-depth analysis using R tools. Think of this as a coding practice more than actually doing descriptive
statistics right. I'm sure I'll get better at that as I progress.

